{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Create .env file with a token generated on HuggingFace:(https://huggingface.co/settings/tokens)\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN=hf_cccccxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Langchain - HuggingFace (repo_id=\"google/flan-t5-large\") - Langchain Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model_kwargs=({\"temperature\":2,\n",
    "              \"max_length\": 640})\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company': 'blue sky', 'product': 'colorful pants', 'text': 'sailor'}\n",
      "sailor\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"You are a naming consultant for new companies.\"\\\n",
    "\"What is a good name for a {company} that make {product}?\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "rs = chain.invoke({\n",
    "  \"company\":\"blue sky\",\n",
    "  \"product\":\"colorful pants\",\n",
    "  })\n",
    "print (rs)\n",
    "print (rs['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain,LLMChain\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model_kwargs=({\"temperature\":0,\n",
    "              \"max_length\": 100})\n",
    "llm = HuggingFaceHub(repo_id=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mvegflora\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mvegflora is a company that makes plants\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': {'Veggie Food'}, 'output': 'vegflora is a company that makes plants'}\n"
     ]
    }
   ],
   "source": [
    "template1 = \"What is a good name for a company that makes {product}?\"\n",
    "prompt1 = PromptTemplate.from_template(template1)\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1)\n",
    "\n",
    "template2 = \"Write a catch phrase for the company:{company}.\"\n",
    "prompt2 = PromptTemplate.from_template(template2)\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2],verbose=True)  \n",
    "catch_phrase = overall_chain.invoke({\"Veggie Food\"})\n",
    "print(catch_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Langchain Action Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wikipedia numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model_kwargs=({\"temperature\":0,\n",
    "              \"max_length\": 100})\n",
    "llm = HuggingFaceHub(repo_id=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "rs = chain.invoke({\n",
    "  \"company\":\"blue sky\",\n",
    "  \"product\":\"colorful pants\",\n",
    "  })\n",
    "print (rs)\n",
    "print (rs['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import get_all_tool_names\n",
    "from langchain.agents import load_tools, AgentExecutor,create_react_agent,initialize_agent\n",
    "import pprint\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"You are a helpful assisant. Answer question: {question}\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "user_prompt = \"Who is Pele?\"\n",
    "\n",
    "# pp = pprint.PrettyPrinter(indent=4)\n",
    "# pp.pprint(get_all_tool_names())\n",
    "# print(get_all_tool_names())\n",
    "\n",
    "\n",
    "\n",
    "tools = load_tools([\"wikipedia\"], llm=llm)\n",
    "# agent = initialize_agent(tools=tools,llm=llm,agent='zero-shot-react-description', verbose=True)\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "rs =   agent_executor.invoke({\"input\": user_prompt})\n",
    "# pprint.pprint(rs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading Documents and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb tiktoken PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load document\n",
    "loader = DirectoryLoader(path=\"./_data/\",\n",
    "                         glob=\"*.pdf\",\n",
    "                         loader_cls=PyMuPDFLoader\n",
    "                         )\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into small chunks\n",
    "# \\n\\n, \\n\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "page_content=\"the App Store amassing thousands of downloads. Additionally, he's competed and won\\nin\\xa0several hackathons around the world including PennApps and NWHacks.\" metadata={'source': '_data/instructor.pdf', 'file_path': '_data/instructor.pdf', 'page': 1, 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Course: LangChain 101 for Beginners (OpenAI / ChatGPT / LLMOps) | Udemy Business', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36', 'producer': 'Skia/PDF m126', 'creationDate': \"D:20240716065224+00'00'\", 'modDate': \"D:20240716065224+00'00'\", 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))\n",
    "print(texts[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model_kwargs=({\"temperature\":0,\n",
    "              \"max_length\": 100})\n",
    "llm = HuggingFaceHub(repo_id=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma MEMORY\n",
    "#embeddings texts\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "store = Chroma.from_documents(texts,embeddings, collection_name=\"langchain101\")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm, retriever=store.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Persisted ChromaDB to disk\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "#instantiate the Chroma object from langchain, using Hugging embedding\n",
    "#DB name = chromadb_langchain101\n",
    "persists_directory = \"./db/chromadb_langchain101\" \n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# Save to disk\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"langchain101\",\n",
    "    persist_directory=persists_directory\n",
    ")\n",
    "\n",
    "#query from Chroma object\n",
    "chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever())\n",
    "# retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models\n"
     ]
    }
   ],
   "source": [
    "question = \"what does LLM stand for?\"\n",
    "response = chain.invoke(question)\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is instructor name? and how many students did learn from him?', 'result': 'Avinash Jain Taught 1 million students how to code'}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"What is instructor name? and how many students did learn from him?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what age did he learn programming?', 'result': '10'}\n"
     ]
    }
   ],
   "source": [
    "question = \"what age did he learn programming?\"\n",
    "print(chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what does he hope?', 'result': 'to equip you with everything you need to embark on your LangChain adventure'}\n"
     ]
    }
   ],
   "source": [
    "question = \"what does he hope?\"\n",
    "print(chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what audience of this course?', 'result': 'Python Developers of any skill level interested in building LLM-powered Python applications with LangChain'}\n"
     ]
    }
   ],
   "source": [
    "question = \"what audience of this course?\"\n",
    "print(chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic programming experience with Python\n"
     ]
    }
   ],
   "source": [
    "question = \"Tell me the requirement of this course?\"\n",
    "response = chain.invoke(question)\n",
    "print(response['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
