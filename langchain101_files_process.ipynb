{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading Documents into ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb tiktoken PyMuPDF langchain langchain-huggingface pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# load document from file_path to memory\n",
    "def load_file(file_path):\n",
    "  pdf_loader = PyMuPDFLoader(file_path)\n",
    "  document = pdf_loader.load()\n",
    "  return document\n",
    "\n",
    "# document = load_file(\"_data/Troubleshooting _ Chroma Docs.pdf\")\n",
    "# print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#split into small chunks\n",
    "# \\n\\n, \\n\n",
    "def chunking_document(document):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "  texts = text_splitter.split_documents(document)\n",
    "  return texts\n",
    "\n",
    "# texts = chunking_document(document)\n",
    "# print(len(texts))\n",
    "# print(texts[len(texts)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model_kwargs=({\"temperature\":0,\n",
    "              \"max_length\": 100})\n",
    "llm = HuggingFaceHub(repo_id=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Persisted ChromaDB to disk\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "#instantiate the Chroma object from langchain, using Hugging embedding\n",
    "def persist_db(texts):\n",
    "    #DB name = chromadb_langchain101\n",
    "    persists_directory = \"./db/chromadb_langchain101\" \n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "    # Save to disk\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=texts,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"langchain101\",\n",
    "        persist_directory=persists_directory\n",
    "    )\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check file_name existed in processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define paths and load processed file\n",
    "data_path = \"./_data/\"\n",
    "processed_file = \"processed.csv\"\n",
    "\n",
    "# Load the processed.csv file, creating it if it doesn't exist\n",
    "if os.path.exists(processed_file):\n",
    "    processed_df = pd.read_csv(processed_file)\n",
    "else:\n",
    "    processed_df = pd.DataFrame(columns=[\"file_name\", \"processed_date\"])\n",
    "\n",
    "# Get the list of all PDF files in the data directory\n",
    "all_files = [f for f in os.listdir(data_path) if f.endswith('.pdf')]\n",
    "\n",
    "# Find the new files by comparing with the processed_df\n",
    "processed_files = set(processed_df[\"file_name\"].tolist())\n",
    "new_files = [f for f in all_files if f not in processed_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                       file_name       processed_date\n",
      "0           0                  instructor.pdf  2024-07-16 15:25:37\n",
      "1           1       UsageGuide_ChromaDocs.pdf  2024-07-19 11:29:06\n",
      "2           2       UsageGuide_ChromaDocs.pdf  2024-07-19 11:31:18\n",
      "3           3  Troubleshooting_ChromaDocs.pdf  2024-07-19 11:31:23\n",
      "4           4     pandas.DataFrame.to_csv.pdf  2024-07-19 11:31:26\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Process each new file\n",
    "for new_file in new_files:\n",
    "    try:\n",
    "        # PDF processing\n",
    "        print(f\"--> {os.path.join(data_path,new_file)}\")\n",
    "        document = load_file(file_path=os.path.join(data_path,new_file))\n",
    "        texts = chunking_document(document)\n",
    "        persist_db(texts=texts)\n",
    "    except:\n",
    "        raise(f\"Cannot process the file: [{new_file}]\")\n",
    "    finally:\n",
    "        row = {\n",
    "            \"file_name\": [new_file],\n",
    "            \"processed_date\": [datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")]\n",
    "            }\n",
    "        df1 = pd.DataFrame(row)\n",
    "        processed_df = pd.concat([processed_df, df1], ignore_index=True)\n",
    "    \n",
    "# Save the updated processed_df back to processed.csv\n",
    "processed_df.to_csv(processed_file, index=True)\n",
    "print(processed_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
